{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk, textblob\n",
    "\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LogisticRegression\n",
    "from sklearn import metrics, datasets, preprocessing\n",
    "\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representing Text Data\n",
    "\n",
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall first create a list of documents. Then we shall tokenize each of them into words. Tokenizing refers to the conversion of documents (as strings) into word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_docs = [\"Here are some very simple basic sentences.\",\n",
    "            \"They won’t be very interesting , I’m afraid.\", \n",
    "            \"The point of these examples is to learn how basic text cleaning works on *very simple* data.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'They won’t be very interesting , I’m afraid.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x26 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 30 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec1 = CountVectorizer()\n",
    "out = vec1.fit_transform(raw_docs)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x26 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 30 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# equivalent approach\n",
    "vec1 = CountVectorizer()\n",
    "vec1.fit(raw_docs)\n",
    "vec1.transform(raw_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sparse vector corresponding to document 1 in `raw_docs` can be extracted with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse representation is to omit values that equate zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.toarray()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The binary values indicate which features (or words) were present in the document. Match up the features found with the vector above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['afraid',\n",
       " 'are',\n",
       " 'basic',\n",
       " 'be',\n",
       " 'cleaning',\n",
       " 'data',\n",
       " 'examples',\n",
       " 'here',\n",
       " 'how',\n",
       " 'interesting',\n",
       " 'is',\n",
       " 'learn',\n",
       " 'of',\n",
       " 'on',\n",
       " 'point',\n",
       " 'sentences',\n",
       " 'simple',\n",
       " 'some',\n",
       " 'text',\n",
       " 'the',\n",
       " 'these',\n",
       " 'they',\n",
       " 'to',\n",
       " 'very',\n",
       " 'won',\n",
       " 'works']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec1.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0],\n",
       "       [1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 1, 1, 0],\n",
       "       [0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,\n",
       "        1, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vocabulary of words has been set up using these three documents. It can be accessed via the following dictionary. The numbers refer to the id of the word in the sparse matrix representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'afraid': 0,\n",
       " 'are': 1,\n",
       " 'basic': 2,\n",
       " 'be': 3,\n",
       " 'cleaning': 4,\n",
       " 'data': 5,\n",
       " 'examples': 6,\n",
       " 'here': 7,\n",
       " 'how': 8,\n",
       " 'interesting': 9,\n",
       " 'is': 10,\n",
       " 'learn': 11,\n",
       " 'of': 12,\n",
       " 'on': 13,\n",
       " 'point': 14,\n",
       " 'sentences': 15,\n",
       " 'simple': 16,\n",
       " 'some': 17,\n",
       " 'text': 18,\n",
       " 'the': 19,\n",
       " 'these': 20,\n",
       " 'they': 21,\n",
       " 'to': 22,\n",
       " 'very': 23,\n",
       " 'won': 24,\n",
       " 'works': 25}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec1.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec1.vocabulary_['afraid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec1.vocabulary_['very']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process pf converting documents into word vectors is known as tokenizing. To obtain the tokenized version of a particular document, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'point',\n",
       " 'of',\n",
       " 'these',\n",
       " 'examples',\n",
       " 'is',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'how',\n",
       " 'basic',\n",
       " 'text',\n",
       " 'cleaning',\n",
       " 'works',\n",
       " 'on',\n",
       " 'very',\n",
       " 'simple',\n",
       " 'data']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec1a = vec1.build_analyzer()\n",
    "vec1a(raw_docs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process is known as tokenzing, whhich is the converting of document-as-strings into word-vectors.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.4261835 , 0.32412354, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.4261835 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.4261835 , 0.32412354, 0.4261835 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.25171084, 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfer = TfidfVectorizer()\n",
    "out2 = tfer.fit_transform(raw_docs)\n",
    "out2.toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.toarray()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector representing the first document now contains weighted values instead of raw frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['afraid',\n",
       " 'are',\n",
       " 'are some',\n",
       " 'basic',\n",
       " 'basic sentences',\n",
       " 'basic text',\n",
       " 'be',\n",
       " 'be very',\n",
       " 'cleaning',\n",
       " 'cleaning works',\n",
       " 'data',\n",
       " 'examples',\n",
       " 'examples is',\n",
       " 'here',\n",
       " 'here are',\n",
       " 'how',\n",
       " 'how basic',\n",
       " 'interesting',\n",
       " 'interesting afraid',\n",
       " 'is',\n",
       " 'is to',\n",
       " 'learn',\n",
       " 'learn how',\n",
       " 'of',\n",
       " 'of these',\n",
       " 'on',\n",
       " 'on very',\n",
       " 'point',\n",
       " 'point of',\n",
       " 'sentences',\n",
       " 'simple',\n",
       " 'simple basic',\n",
       " 'simple data',\n",
       " 'some',\n",
       " 'some very',\n",
       " 'text',\n",
       " 'text cleaning',\n",
       " 'the',\n",
       " 'the point',\n",
       " 'these',\n",
       " 'these examples',\n",
       " 'they',\n",
       " 'they won',\n",
       " 'to',\n",
       " 'to learn',\n",
       " 'very',\n",
       " 'very interesting',\n",
       " 'very simple',\n",
       " 'won',\n",
       " 'won be',\n",
       " 'works',\n",
       " 'works on']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfer_ng = TfidfVectorizer(ngram_range=(1,2))\n",
    "# also have two words.\n",
    "out3 = tfer_ng.fit_transform(raw_docs)\n",
    "tfer_ng.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with TextBlob\n",
    "\n",
    "### TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextBlob is an interesting python library that offer simple API to access its methods and perform some basic NLP tasks.\n",
    "\n",
    "Let's take a look at how to use textblob to do the same functions as we talked about previously. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the next step, be sure to run the following commands from a terminal, where the virtual environment has been activated:\n",
    "\n",
    "`$ /Applications/Python\\ 3.6/Install\\ Certificates.command`\n",
    "\n",
    "`$ python -m textblob.download_corpora`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4352345435'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\"4352345\" \"435\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "doc1 = (\"Data science is pretty awesome! \\n \"\n",
    "       \"There's so many discussions that we can hold about it.\")\n",
    "blob = TextBlob(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence(\"There's so many discussions that we can hold about it.\")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['There', \"'s\", 'so', 'many', 'discussions', 'that', 'we', 'can', 'hold', 'about', 'it'])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.sentences[1].words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sentence(\"Data science is pretty awesome!\"), Sentence(\"There's so many discussions that we can hold about it.\")]\n",
      "Data\n",
      "science\n",
      "is\n",
      "pretty\n",
      "awesome\n"
     ]
    }
   ],
   "source": [
    "print(blob.sentences)\n",
    "for words in blob.sentences[0].words:\n",
    "    print (words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we can tokenize the textblob into sentences, and further into words.  We can perform another function called noun phase extraction that extracts just the noun phrases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n",
      "pretty awesome\n"
     ]
    }
   ],
   "source": [
    "#noun phrase extraction \n",
    "for np in blob.noun_phrases:\n",
    "    print (np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"Data science is pretty awesome!\"),\n",
       " Sentence(\"There's so many discussions that we can hold about it.\")]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at how lemmatization can be done.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discussions\n",
      "discussion\n",
      "science\n",
      "sciences\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lemmatization \n",
    "print (blob.sentences[1].words[4])\n",
    "print (blob.sentences[1].words[4].singularize())\n",
    "print (blob.sentences[0].words[1])\n",
    "print (blob.sentences[0].words[1].pluralize())\n",
    "#lemmatizing single words\n",
    "from textblob import Word\n",
    "w = Word('running')\n",
    "w.lemmatize(\"v\") ## v here represents verb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.5833333333333334, subjectivity=0.8333333333333334)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polarity: posive\n",
    "# subjectivity: subjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.5833333333333334, subjectivity=0.8333333333333334, assessments=[(['pretty'], 0.25, 1.0, None), (['awesome', '!'], 1.0, 1.0, None), (['many'], 0.5, 0.5, None)])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.sentiment_assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
